# encoding: utf-8
"""
è§£æJSONæ–‡ä»¶ï¼Œè·å–ç¬”è®°çš„å®Œæ•´ä¿¡æ¯å¹¶ä¿å­˜
å®ç°æ­¥éª¤2ï¼šè¯»å–JSONæ–‡ä»¶ï¼Œçˆ¬å–å®Œæ•´çš„ç¬”è®°ä¿¡æ¯ï¼ˆåŒ…æ‹¬å›¾ç‰‡ã€è§†é¢‘ã€æ–‡å­—ã€è¯„è®ºï¼‰
"""

import json
import os
from datetime import datetime
from loguru import logger
from apis.xhs_pc_apis import XHS_Apis
from xhs_utils.common_util import init
from xhs_utils.data_util import handle_note_info, download_note, handle_comment_info


class JsonToFullData:
    """
    è§£æJSONæ–‡ä»¶å¹¶è·å–å®Œæ•´ç¬”è®°ä¿¡æ¯çš„ç±»
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–ç±»å®ä¾‹
        """
        self.xhs_apis = XHS_Apis()
        
    def parse_json_file(self, json_file_path: str):
        """
        è§£æJSONæ–‡ä»¶ï¼Œæå–ç¬”è®°URLåˆ—è¡¨
        
        :param json_file_path: JSONæ–‡ä»¶è·¯å¾„
        :return: æˆåŠŸçŠ¶æ€, æ¶ˆæ¯, ç¬”è®°URLåˆ—è¡¨
        """
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if 'notes' not in data:
                return False, 'JSONæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘noteså­—æ®µ', []
            
            notes = data['notes']
            note_urls = []
            
            for note in notes:
                if 'note_url' in note:
                    note_urls.append(note['note_url'])
                elif 'note_id' in note and 'xsec_token' in note:
                    # æ ¹æ®note_idå’Œxsec_tokenæ„å»ºURL
                    note_url = f"https://www.xiaohongshu.com/explore/{note['note_id']}?xsec_token={note['xsec_token']}"
                    note_urls.append(note_url)
            
            logger.info(f'ä» {json_file_path} è§£æå‡º {len(note_urls)} ä¸ªç¬”è®°URL')
            return True, f'æˆåŠŸè§£æ {len(note_urls)} ä¸ªç¬”è®°URL', note_urls
            
        except Exception as e:
            error_msg = f'è§£æJSONæ–‡ä»¶å¤±è´¥: {str(e)}'
            logger.error(error_msg)
            return False, error_msg, []
    
    def get_note_full_info(self, note_url: str, cookies_str: str, proxies: dict = None, include_comments: bool = True):
        """
        è·å–å•ä¸ªç¬”è®°çš„å®Œæ•´ä¿¡æ¯
        
        :param note_url: ç¬”è®°URL
        :param cookies_str: å°çº¢ä¹¦cookieså­—ç¬¦ä¸²
        :param proxies: ä»£ç†è®¾ç½®
        :param include_comments: æ˜¯å¦åŒ…å«è¯„è®ºæ•°æ®
        :return: æˆåŠŸçŠ¶æ€, æ¶ˆæ¯, ç¬”è®°å®Œæ•´ä¿¡æ¯
        """
        try:
            # è·å–ç¬”è®°åŸºæœ¬ä¿¡æ¯
            success, msg, note_info = self.xhs_apis.get_note_info(note_url, cookies_str, proxies)
            if not success:
                return False, f'è·å–ç¬”è®°ä¿¡æ¯å¤±è´¥: {msg}', None
            
            # å¤„ç†ç¬”è®°ä¿¡æ¯
            note_info = note_info['data']['items'][0]
            note_info['url'] = note_url
            processed_note = handle_note_info(note_info)
            
            # è·å–è¯„è®ºæ•°æ®
            if include_comments:
                try:
                    logger.info(f'å¼€å§‹è·å–è¯„è®ºæ•°æ®: {note_url}')
                    comment_success, comment_msg, comments = self.xhs_apis.get_note_all_comment(
                        note_url, cookies_str, proxies
                    )
                    logger.info(f'è¯„è®ºAPIè¿”å›: success={comment_success}, msg={comment_msg}, comments_count={len(comments) if comments else 0}')
                    
                    if comment_success and comments:
                        processed_comments = []
                        for comment in comments:
                            comment['note_id'] = processed_note['note_id']
                            comment['note_url'] = note_url
                            processed_comment = handle_comment_info(comment)
                            processed_comments.append(processed_comment)
                        processed_note['comments'] = processed_comments
                        logger.info(f'è·å–è¯„è®ºæ•°æ®æˆåŠŸï¼Œå…± {len(processed_comments)} æ¡è¯„è®º')
                    else:
                        processed_note['comments'] = []
                        logger.warning(f'è·å–è¯„è®ºæ•°æ®å¤±è´¥: success={comment_success}, msg={comment_msg}')
                        # è´¦å·å¯èƒ½è¢«é™åˆ¶ï¼Œè·³è¿‡è¯„è®ºè·å–
                except Exception as e:
                    processed_note['comments'] = []
                    logger.warning(f'è·å–è¯„è®ºæ•°æ®å¼‚å¸¸: {str(e)}')
            else:
                processed_note['comments'] = []
            
            # æ·»åŠ è·å–æ—¶é—´æˆ³
            processed_note['crawl_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            return True, 'è·å–ç¬”è®°å®Œæ•´ä¿¡æ¯æˆåŠŸ', processed_note
            
        except Exception as e:
            error_msg = f'è·å–ç¬”è®°å®Œæ•´ä¿¡æ¯å¤±è´¥: {str(e)}'
            logger.error(error_msg)
            return False, error_msg, None
    
    def process_json_to_full_data(self, json_file_path: str, cookies_str: str, 
                                 output_dir: str = None, include_comments: bool = True, 
                                 download_media: bool = True, save_format: str = 'json',
                                 proxies: dict = None):
        """
        å¤„ç†JSONæ–‡ä»¶ï¼Œè·å–æ‰€æœ‰ç¬”è®°çš„å®Œæ•´ä¿¡æ¯å¹¶ä¿å­˜
        
        :param json_file_path: è¾“å…¥çš„JSONæ–‡ä»¶è·¯å¾„
        :param cookies_str: å°çº¢ä¹¦cookieså­—ç¬¦ä¸²
        :param output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è‡ªåŠ¨ç”Ÿæˆ
        :param include_comments: æ˜¯å¦åŒ…å«è¯„è®ºæ•°æ®
        :param download_media: æ˜¯å¦ä¸‹è½½åª’ä½“æ–‡ä»¶ï¼ˆå›¾ç‰‡ã€è§†é¢‘ï¼‰
        :param save_format: ä¿å­˜æ ¼å¼ 'json', 'excel', 'all'
        :param proxies: ä»£ç†è®¾ç½®
        :return: æˆåŠŸçŠ¶æ€, æ¶ˆæ¯, å¤„ç†ç»“æœç»Ÿè®¡
        """
        try:
            # è§£æJSONæ–‡ä»¶
            parse_success, parse_msg, note_urls = self.parse_json_file(json_file_path)
            if not parse_success:
                return False, parse_msg, {}
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            if output_dir is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                json_filename = os.path.splitext(os.path.basename(json_file_path))[0]
                output_dir = f"full_data_{json_filename}_{timestamp}"
            
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
            
            # åˆ›å»ºåª’ä½“æ–‡ä»¶ç›®å½•
            if download_media:
                media_dir = os.path.join(output_dir, "media_files")
                if not os.path.exists(media_dir):
                    os.makedirs(media_dir)
            
            # å¤„ç†æ¯ä¸ªç¬”è®°
            successful_notes = []
            failed_notes = []
            all_comments = []
            
            for i, note_url in enumerate(note_urls, 1):
                logger.info(f'æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(note_urls)} ä¸ªç¬”è®°: {note_url}')
                
                success, msg, full_note_info = self.get_note_full_info(
                    note_url, cookies_str, proxies, include_comments
                )
                
                if success and full_note_info:
                    successful_notes.append(full_note_info)
                    
                    # æ”¶é›†è¯„è®ºæ•°æ®
                    if include_comments and 'comments' in full_note_info:
                        all_comments.extend(full_note_info['comments'])
                    
                    # ä¸‹è½½åª’ä½“æ–‡ä»¶
                    if download_media:
                        try:
                            base_path = {'media': media_dir, 'excel': output_dir}
                            download_note(full_note_info, media_dir, 'media')
                            logger.info(f'åª’ä½“æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {full_note_info["title"]}')
                        except Exception as e:
                            logger.warning(f'åª’ä½“æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}')
                    
                    # ä¿å­˜å•ä¸ªç¬”è®°çš„å®Œæ•´ä¿¡æ¯ä¸ºJSON
                    if save_format in ['json', 'all']:
                        note_json_file = os.path.join(
                            output_dir, 
                            f"note_{full_note_info['note_id']}_full.json"
                        )
                        with open(note_json_file, 'w', encoding='utf-8') as f:
                            json.dump(full_note_info, f, ensure_ascii=False, indent=2)
                else:
                    failed_notes.append({
                        'url': note_url,
                        'error': msg
                    })
                    logger.error(f'å¤„ç†å¤±è´¥: {msg}')
            
            # ä¿å­˜æ±‡æ€»æ•°æ®
            summary_data = {
                'process_info': {
                    'source_json': json_file_path,
                    'total_notes': len(note_urls),
                    'successful_notes': len(successful_notes),
                    'failed_notes': len(failed_notes),
                    'total_comments': len(all_comments),
                    'include_comments': include_comments,
                    'download_media': download_media,
                    'process_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                },
                'successful_notes': successful_notes,
                'failed_notes': failed_notes
            }
            
            # ä¿å­˜æ±‡æ€»JSONæ–‡ä»¶
            if save_format in ['json', 'all']:
                summary_file = os.path.join(output_dir, "summary_all_notes.json")
                with open(summary_file, 'w', encoding='utf-8') as f:
                    json.dump(summary_data, f, ensure_ascii=False, indent=2)
                logger.success(f'æ±‡æ€»JSONæ–‡ä»¶ä¿å­˜åˆ°: {summary_file}')
            
            # ä¿å­˜ä¸ºExcelæ ¼å¼
            if save_format in ['excel', 'all']:
                from xhs_utils.data_util import save_to_xlsx
                
                # ä¿å­˜ç¬”è®°æ•°æ®åˆ°Excel
                if successful_notes:
                    excel_file = os.path.join(output_dir, "notes_data.xlsx")
                    save_to_xlsx(successful_notes, excel_file)
                    logger.success(f'ç¬”è®°Excelæ–‡ä»¶ä¿å­˜åˆ°: {excel_file}')
                
                # ä¿å­˜è¯„è®ºæ•°æ®åˆ°Excel
                if include_comments and all_comments:
                    comment_excel_file = os.path.join(output_dir, "comments_data.xlsx")
                    save_to_xlsx(all_comments, comment_excel_file, 'comment')
                    logger.success(f'è¯„è®ºExcelæ–‡ä»¶ä¿å­˜åˆ°: {comment_excel_file}')
            
            # ä¿å­˜å¤„ç†ç»“æœç»Ÿè®¡
            result_stats = {
                'total_notes': len(note_urls),
                'successful_notes': len(successful_notes),
                'failed_notes': len(failed_notes),
                'success_rate': len(successful_notes) / len(note_urls) * 100 if note_urls else 0,
                'total_comments': len(all_comments),
                'output_directory': output_dir
            }
            
            logger.success(f'å¤„ç†å®Œæˆï¼æˆåŠŸ: {len(successful_notes)}, å¤±è´¥: {len(failed_notes)}')
            logger.success(f'ç»“æœä¿å­˜åˆ°ç›®å½•: {output_dir}')
            
            return True, 'å¤„ç†å®Œæˆ', result_stats
            
        except Exception as e:
            error_msg = f'å¤„ç†JSONæ–‡ä»¶å¤±è´¥: {str(e)}'
            logger.error(error_msg)
            return False, error_msg, {}
    
    def batch_process_json_files(self, json_files: list, cookies_str: str, 
                               output_base_dir: str = "batch_full_data", **kwargs):
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªJSONæ–‡ä»¶
        
        :param json_files: JSONæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        :param cookies_str: å°çº¢ä¹¦cookieså­—ç¬¦ä¸²
        :param output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        :param kwargs: å…¶ä»–å¤„ç†å‚æ•°
        :return: æ‰¹é‡å¤„ç†ç»“æœ
        """
        if not os.path.exists(output_base_dir):
            os.makedirs(output_base_dir)
        
        batch_results = []
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for json_file in json_files:
            logger.info(f'å¼€å§‹å¤„ç†JSONæ–‡ä»¶: {json_file}')
            
            # ä¸ºæ¯ä¸ªJSONæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
            json_name = os.path.splitext(os.path.basename(json_file))[0]
            output_dir = os.path.join(output_base_dir, f"{json_name}_{timestamp}")
            
            success, msg, stats = self.process_json_to_full_data(
                json_file, cookies_str, output_dir, **kwargs
            )
            
            batch_results.append({
                'json_file': json_file,
                'success': success,
                'message': msg,
                'stats': stats,
                'output_dir': output_dir if success else None
            })
        
        # ä¿å­˜æ‰¹é‡å¤„ç†æ±‡æ€»ç»“æœ
        batch_summary = {
            'batch_info': {
                'total_files': len(json_files),
                'successful_files': len([r for r in batch_results if r['success']]),
                'total_notes_processed': sum([r['stats'].get('total_notes', 0) for r in batch_results]),
                'total_successful_notes': sum([r['stats'].get('successful_notes', 0) for r in batch_results]),
                'process_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            },
            'results': batch_results
        }
        
        batch_summary_file = os.path.join(output_base_dir, f"batch_summary_{timestamp}.json")
        with open(batch_summary_file, 'w', encoding='utf-8') as f:
            json.dump(batch_summary, f, ensure_ascii=False, indent=2)
        
        logger.success(f'æ‰¹é‡å¤„ç†å®Œæˆï¼Œæ±‡æ€»ç»“æœä¿å­˜åˆ°: {batch_summary_file}')
        return batch_results


if __name__ == '__main__':
    """
    è§£æJSONæ–‡ä»¶å¹¶è·å–å®Œæ•´ç¬”è®°ä¿¡æ¯çš„ç¤ºä¾‹ä½¿ç”¨
    """
    # åˆå§‹åŒ–é…ç½®
    cookies_str, base_path = init()
    json_processor = JsonToFullData()
    
    # ç¤ºä¾‹ï¼šå¤„ç†å•ä¸ªJSONæ–‡ä»¶
    json_file_path = "search_results/search_æ—¥æœ¬æ–™ç†_20250905_183800.json"  # è¯·ä¿®æ”¹ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if os.path.exists(json_file_path):
        success, msg, stats = json_processor.process_json_to_full_data(
            json_file_path=json_file_path,
            cookies_str=cookies_str,
            include_comments=True,  # åŒ…å«è¯„è®ºæ•°æ®
            download_media=True,    # ä¸‹è½½åª’ä½“æ–‡ä»¶
            save_format='all'       # ä¿å­˜ä¸ºJSONå’ŒExcelæ ¼å¼
        )
        
        if success:
            print(f"âœ… å¤„ç†æˆåŠŸ: {msg}")
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}")
        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {msg}")
    else:
        print(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_file_path}")
        print("è¯·å…ˆè¿è¡Œ search_to_json.py ç”ŸæˆJSONæ–‡ä»¶")
    
    # æ‰¹é‡å¤„ç†ç¤ºä¾‹ï¼ˆæ³¨é‡Šæ‰ï¼Œéœ€è¦æ—¶å¯ç”¨ï¼‰
    """
    json_files = [
        "search_results/search_æ—¥æœ¬æ–™ç†_20240101_120000.json",
        "search_results/search_æ„å¤§åˆ©é¢_20240101_120000.json"
    ]
    
    existing_files = [f for f in json_files if os.path.exists(f)]
    if existing_files:
        results = json_processor.batch_process_json_files(
            json_files=existing_files,
            cookies_str=cookies_str,
            include_comments=True,
            download_media=True,
            save_format='all'
        )
        
        for result in results:
            status = "âœ…" if result['success'] else "âŒ"
            file_name = os.path.basename(result['json_file'])
            print(f"{status} {file_name}: {result['message']}")
    """