# encoding: utf-8
"""
è§£æJSONæ–‡ä»¶ï¼Œè·å–ç¬”è®°çš„å®Œæ•´ä¿¡æ¯å¹¶ä¿å­˜
å®ç°æ­¥éª¤2ï¼šè¯»å–JSONæ–‡ä»¶ï¼Œçˆ¬å–å®Œæ•´çš„ç¬”è®°ä¿¡æ¯ï¼ˆåŒ…æ‹¬å›¾ç‰‡ã€è§†é¢‘ã€æ–‡å­—ã€è¯„è®ºï¼‰
"""

import json
import os
import time
from datetime import datetime
from loguru import logger
from apis.xhs_pc_apis import XHS_Apis
from xhs_utils.common_util import init
from xhs_utils.data_util import handle_note_info, download_note, handle_comment_info


class JsonToFullData:
    """
    è§£æJSONæ–‡ä»¶å¹¶è·å–å®Œæ•´ç¬”è®°ä¿¡æ¯çš„ç±»
    """

    def __init__(self, cookie_pool=None):
        """
        åˆå§‹åŒ–ç±»å®ä¾‹

        :param cookie_pool: Cookieæ± å®ä¾‹ï¼Œç”¨äºè‡ªåŠ¨åˆ‡æ¢Cookieé‡è¯•
        """
        self.xhs_apis = XHS_Apis()
        self.cookie_pool = cookie_pool
        
    def parse_json_file(self, json_file_path: str):
        """
        è§£æJSONæ–‡ä»¶ï¼Œæå–ç¬”è®°URLåˆ—è¡¨
        
        :param json_file_path: JSONæ–‡ä»¶è·¯å¾„
        :return: æˆåŠŸçŠ¶æ€, æ¶ˆæ¯, ç¬”è®°URLåˆ—è¡¨
        """
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if 'notes' not in data:
                return False, 'JSONæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘noteså­—æ®µ', []
            
            notes = data['notes']
            note_urls = []
            
            for note in notes:
                if 'note_url' in note:
                    note_urls.append(note['note_url'])
                elif 'note_id' in note and 'xsec_token' in note:
                    # æ ¹æ®note_idå’Œxsec_tokenæ„å»ºURL
                    note_url = f"https://www.xiaohongshu.com/explore/{note['note_id']}?xsec_token={note['xsec_token']}"
                    note_urls.append(note_url)
            
            logger.info(f'ä» {json_file_path} è§£æå‡º {len(note_urls)} ä¸ªç¬”è®°URL')
            return True, f'æˆåŠŸè§£æ {len(note_urls)} ä¸ªç¬”è®°URL', note_urls

        except Exception as e:
            error_msg = f'è§£æJSONæ–‡ä»¶å¤±è´¥: {str(e)}'
            logger.error(error_msg)
            return False, error_msg, []

    def get_with_cookie_pool_retry(self, api_func, *args, **kwargs):
        """
        ä½¿ç”¨Cookieæ± æ‰€æœ‰è´¦å·è¿›è¡Œé‡è¯•
        éå†æ•´ä¸ªæ± ï¼Œåªæœ‰æ‰€æœ‰Cookieéƒ½å¤±è´¥æ‰æ”¾å¼ƒ

        :param api_func: è¦è°ƒç”¨çš„APIæ–¹æ³•
        :param args: APIæ–¹æ³•çš„ä½ç½®å‚æ•°
        :param kwargs: APIæ–¹æ³•çš„å…³é”®å­—å‚æ•°ï¼ˆä¸åŒ…å«cookies_strï¼‰
        :return: success, msg, data, accountï¼ˆä½¿ç”¨çš„è´¦å·ï¼‰
        """
        if not self.cookie_pool:
            # å¦‚æœæ²¡æœ‰Cookieæ± ï¼Œä½¿ç”¨ä¼ å…¥çš„cookies_str
            if 'cookies_str' in kwargs:
                try:
                    success, msg, data = api_func(*args, **kwargs)
                    return success, msg, data, None
                except Exception as e:
                    return False, str(e), None, None
            else:
                return False, "æœªæä¾›Cookieä¸”Cookieæ± ä¸å¯ç”¨", None, None

        tried_cookie_ids = set()  # è®°å½•å·²å°è¯•çš„Cookie ID
        total_accounts = len(self.cookie_pool.accounts)

        if total_accounts == 0:
            logger.error("Cookieæ± ä¸­æ²¡æœ‰å¯ç”¨è´¦å·")
            return False, "Cookieæ± ä¸ºç©º", None, None

        logger.info(f"Cookieæ± å…±æœ‰ {total_accounts} ä¸ªè´¦å·å¯ä¾›é‡è¯•")

        wait_rounds = 0  # ç­‰å¾…è½®æ•°è®¡æ•°å™¨
        max_wait_rounds = 3  # æœ€å¤§ç­‰å¾…è½®æ•°

        while len(tried_cookie_ids) < total_accounts:
            # è·å–å¯ç”¨è´¦å·
            account = self.cookie_pool.get_available_account()

            if not account:
                # å¦‚æœæ²¡æœ‰å¯ç”¨è´¦å·ï¼Œæ£€æŸ¥æ˜¯å¦æ‰€æœ‰è´¦å·éƒ½å·²å°è¯•è¿‡
                if len(tried_cookie_ids) >= total_accounts:
                    logger.error("æ‰€æœ‰Cookieè´¦å·å‡å·²å°è¯•")
                    break

                # å¦‚æœè¿˜æœ‰æœªå°è¯•çš„è´¦å·ï¼Œä½†æš‚æ—¶éƒ½ä¸å¯ç”¨ï¼ˆå¯èƒ½åœ¨å†·å´ä¸­ï¼‰
                if wait_rounds < max_wait_rounds:
                    wait_rounds += 1
                    wait_time = 2  # ç­‰å¾…2ç§’è®©è´¦å·å†·å´
                    logger.warning(f"æ‰€æœ‰è´¦å·æš‚æ—¶ä¸å¯ç”¨ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯• (ç¬¬ {wait_rounds}/{max_wait_rounds} è½®)")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.error(f"ç­‰å¾… {max_wait_rounds} è½®åä»æ— å¯ç”¨è´¦å·")
                    break

            # è·³è¿‡å·²å°è¯•çš„Cookie
            if account.cookie_id in tried_cookie_ids:
                continue

            tried_cookie_ids.add(account.cookie_id)
            logger.info(f"å°è¯•Cookieè´¦å·: {account.name} ({len(tried_cookie_ids)}/{total_accounts})")

            try:
                # è°ƒç”¨APIï¼Œä¼ å…¥Cookie
                success, msg, data = api_func(*args, cookies_str=account.cookie_str, **kwargs)

                if success:
                    self.cookie_pool.mark_account_success(account.cookie_id)
                    logger.info(f"âœ… Cookie {account.name} è¯·æ±‚æˆåŠŸ")
                    return success, msg, data, account
                else:
                    self.cookie_pool.mark_account_error(account.cookie_id, msg)
                    logger.warning(f"âŒ Cookie {account.name} å¤±è´¥: {msg}ï¼Œåˆ‡æ¢ä¸‹ä¸€ä¸ª")

            except Exception as e:
                self.cookie_pool.mark_account_error(account.cookie_id, str(e))
                logger.warning(f"âŒ Cookie {account.name} å¼‚å¸¸: {e}ï¼Œåˆ‡æ¢ä¸‹ä¸€ä¸ª")

        # æ‰€æœ‰Cookieéƒ½å¤±è´¥
        logger.error(f"æ‰€æœ‰ {total_accounts} ä¸ªCookieè´¦å·å‡å·²å°è¯•å¤±è´¥")
        return False, f"æ‰€æœ‰Cookieè´¦å·({total_accounts}ä¸ª)å‡å¤±è´¥", None, None

    def save_comments_streaming(self, note_id: str, xsec_token: str, output_file: str, proxies: dict = None):
        """
        æµå¼è·å–è¯„è®ºï¼Œæ¯é¡µç«‹å³ä¿å­˜åˆ°JSONLæ–‡ä»¶
        å¤±è´¥æ—¶éå†æ‰€æœ‰Cookieé‡è¯•

        :param note_id: ç¬”è®°ID
        :param xsec_token: xsec_tokenå‚æ•°
        :param output_file: è¾“å‡ºçš„JSONLæ–‡ä»¶è·¯å¾„
        :param proxies: ä»£ç†è®¾ç½®
        :return: è·å–åˆ°çš„æ€»è¯„è®ºæ•°
        """
        cursor = ''
        page = 0
        total_comments = 0

        # åˆ›å»ºæˆ–æ¸…ç©ºè¯„è®ºæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            pass

        logger.info(f"å¼€å§‹æµå¼è·å–è¯„è®º: note_id={note_id}")

        while True:
            page += 1
            logger.info(f"æ­£åœ¨è·å–ç¬¬ {page} é¡µè¯„è®º...")

            # ä½¿ç”¨Cookieæ± å…¨éå†é‡è¯•
            success, msg, res_json, account = self.get_with_cookie_pool_retry(
                self.xhs_apis.get_note_out_comment,
                note_id, cursor, xsec_token,
                proxies=proxies
            )

            if not success:
                logger.error(f"ç¬¬ {page} é¡µè·å–å¤±è´¥ï¼ˆæ‰€æœ‰Cookieå·²å°è¯•ï¼‰: {msg}")
                break

            # æ£€æŸ¥è¿”å›æ•°æ®ç»“æ„
            if not res_json or 'data' not in res_json:
                logger.warning(f"ç¬¬ {page} é¡µè¿”å›æ•°æ®å¼‚å¸¸ï¼Œåœæ­¢è·å–")
                break

            data = res_json.get('data', {})

            # æ£€æŸ¥æ˜¯å¦æœ‰commentså­—æ®µ
            if 'comments' not in data:
                logger.warning(f"ç¬¬ {page} é¡µè¿”å›dataä¸­æ²¡æœ‰commentså­—æ®µï¼Œåœæ­¢è·å–")
                logger.debug(f"è¿”å›æ•°æ®: {res_json}")
                break

            comments = data['comments']
            has_more = data.get('has_more', False)

            # âœ… è·å–å®Œæ•´å­è¯„è®ºåå†ä¿å­˜ï¼ˆæ”¯æŒå¤šå±‚çº§ï¼‰
            if comments:
                logger.info(f"ç¬¬ {page} é¡µè·å–åˆ° {len(comments)} æ¡ä¸€çº§è¯„è®ºï¼Œå¼€å§‹è·å–æ‰€æœ‰å±‚çº§çš„å­è¯„è®º...")

                # å®šä¹‰Cookieæä¾›å‡½æ•°
                def get_cookie_for_comment():
                    """ä¸ºè¯„è®ºè·å–æä¾›Cookieï¼ˆè‡ªåŠ¨ä½¿ç”¨Cookieæ± ï¼‰"""
                    if account and account.cookie_str:
                        return True, account.cookie_str
                    elif self.cookie_pool:
                        temp_account = self.cookie_pool.get_available_account()
                        if temp_account:
                            return True, temp_account.cookie_str
                    return False, None

                # å¤„ç†æ¯æ¡ä¸€çº§è¯„è®ºï¼Œè·å–æ‰€æœ‰å±‚çº§çš„å­è¯„è®º
                for idx, comment in enumerate(comments, 1):
                    sub_count = comment.get('sub_comment_count', 0)
                    logger.debug(f"  [{idx}/{len(comments)}] æ£€æŸ¥è¯„è®º {comment.get('id', 'N/A')[:20]}, sub_comment_count={sub_count} (ç±»å‹:{type(sub_count).__name__})")

                    if isinstance(sub_count, str):
                        sub_count = int(sub_count) if sub_count.isdigit() else 0
                        logger.debug(f"  è½¬æ¢å sub_count={sub_count}")

                    if sub_count > 0:
                        logger.info(f"  [{idx}/{len(comments)}] è·å–è¯„è®ºçš„å¤šå±‚çº§å­è¯„è®ºï¼ˆé¢„æœŸ{sub_count}æ¡ï¼‰...")

                        try:
                            # ä½¿ç”¨æ–°æ–¹æ³•è·å–æ‰€æœ‰å±‚çº§çš„å­è¯„è®º
                            success, msg, full_comment = self.xhs_apis.get_note_all_inner_comment_with_provider(
                                comment, xsec_token, get_cookie_for_comment, proxies,
                                level=2, max_level=10  # æœ€å¤šæ”¯æŒ10å±‚è¯„è®º
                            )

                            if success:
                                comments[idx-1] = full_comment
                                # ç»Ÿè®¡å®é™…è·å–çš„è¯„è®ºæ•°ï¼ˆåŒ…æ‹¬æ‰€æœ‰å±‚çº§ï¼‰
                                def count_recursive(c):
                                    count = len(c.get('sub_comments', []))
                                    for sub in c.get('sub_comments', []):
                                        count += count_recursive(sub)
                                    return count

                                actual_count = count_recursive(full_comment)
                                logger.info(f"  âœ… æˆåŠŸè·å– {actual_count} æ¡å¤šå±‚çº§å­è¯„è®º")
                            else:
                                logger.warning(f"  âš ï¸ å­è¯„è®ºè·å–å¤±è´¥: {msg}ï¼Œå°†ä¿å­˜ä¸å®Œæ•´æ•°æ®")

                        except Exception as e:
                            logger.warning(f"  âš ï¸ å¤„ç†è¯„è®ºå¼‚å¸¸: {e}ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€æ¡")

                # ç»Ÿè®¡çœŸå®è¯„è®ºæ•°ï¼ˆåŒ…æ‹¬æ‰€æœ‰å±‚çº§ï¼‰
                def count_all_levels(comment_list):
                    """é€’å½’ç»Ÿè®¡æ‰€æœ‰å±‚çº§çš„è¯„è®ºæ•°"""
                    count = len(comment_list)
                    for c in comment_list:
                        if 'sub_comments' in c and c['sub_comments']:
                            count += count_all_levels(c['sub_comments'])
                    return count

                page_total = count_all_levels(comments)

                # ä¿å­˜åˆ°JSONLæ–‡ä»¶
                with open(output_file, 'a', encoding='utf-8') as f:
                    for comment in comments:
                        comment['note_id'] = note_id
                        f.write(json.dumps(comment, ensure_ascii=False) + '\n')
                    f.flush()  # ç«‹å³åˆ·æ–°åˆ°ç£ç›˜

                total_comments += page_total
                logger.info(f"âœ… ç¬¬ {page} é¡µå·²ä¿å­˜ï¼ˆä¸€çº§: {len(comments)}ï¼Œæ€»è®¡æ‰€æœ‰å±‚çº§: {page_total}ï¼Œç´¯è®¡: {total_comments}ï¼‰")
            else:
                logger.info(f"ç¬¬ {page} é¡µæ²¡æœ‰è¯„è®ºæ•°æ®ï¼Œåœæ­¢è·å–")
                break

            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤š
            if not has_more:
                logger.info(f"has_moreä¸ºFalseï¼Œè¯„è®ºè·å–å®Œæˆ")
                break

            # æ›´æ–°cursor
            if 'cursor' in data:
                cursor = str(data['cursor'])
                logger.debug(f"ä¸‹ä¸€é¡µcursor: {cursor}")
            else:
                logger.info("æ²¡æœ‰cursorå­—æ®µï¼Œåœæ­¢è·å–")
                break

            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)

        logger.info(f"è¯„è®ºè·å–å®Œæˆï¼Œå…± {total_comments} æ¡è¯„è®ºï¼ˆåŒ…å«æ‰€æœ‰å±‚çº§ï¼‰ä¿å­˜åˆ°: {output_file}")
        return total_comments

    def get_note_full_info(self, note_url: str, cookies_str: str = None, output_dir: str = None,
                           proxies: dict = None, include_comments: bool = True):
        """
        è·å–å•ä¸ªç¬”è®°çš„å®Œæ•´ä¿¡æ¯ï¼ˆæ”¯æŒåˆ†æ­¥ä¿å­˜å’ŒCookieæ± é‡è¯•ï¼‰

        :param note_url: ç¬”è®°URL
        :param cookies_str: å°çº¢ä¹¦cookieså­—ç¬¦ä¸²ï¼ˆå¦‚æœä½¿ç”¨Cookieæ± åˆ™å¯é€‰ï¼‰
        :param output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœæŒ‡å®šåˆ™åˆ†æ­¥ä¿å­˜æ–‡ä»¶ï¼‰
        :param proxies: ä»£ç†è®¾ç½®
        :param include_comments: æ˜¯å¦åŒ…å«è¯„è®ºæ•°æ®
        :return: æˆåŠŸçŠ¶æ€, æ¶ˆæ¯, ç¬”è®°å®Œæ•´ä¿¡æ¯
        """
        try:
            # æ­¥éª¤1: è·å–ç¬”è®°åŸºæœ¬ä¿¡æ¯ï¼ˆä½¿ç”¨Cookieæ± é‡è¯•ï¼‰
            logger.info(f'å¼€å§‹è·å–ç¬”è®°åŸºæœ¬ä¿¡æ¯: {note_url}')

            if self.cookie_pool:
                # ä½¿ç”¨Cookieæ± é‡è¯•
                success, msg, note_info, account = self.get_with_cookie_pool_retry(
                    self.xhs_apis.get_note_info,
                    note_url,
                    proxies=proxies
                )
            else:
                # ç›´æ¥ä½¿ç”¨æä¾›çš„Cookie
                success, msg, note_info = self.xhs_apis.get_note_info(note_url, cookies_str, proxies)
                account = None

            if not success:
                return False, f'è·å–ç¬”è®°ä¿¡æ¯å¤±è´¥: {msg}', None

            # å¤„ç†ç¬”è®°ä¿¡æ¯
            note_info = note_info['data']['items'][0]
            note_info['url'] = note_url
            processed_note = handle_note_info(note_info)
            note_id = processed_note['note_id']

            # æ·»åŠ è·å–æ—¶é—´æˆ³
            processed_note['crawl_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            # âœ… æ­¥éª¤2: ç«‹å³ä¿å­˜åŸºæœ¬ä¿¡æ¯ï¼ˆå¦‚æœæŒ‡å®šäº†output_dirï¼‰
            if output_dir:
                basic_file = os.path.join(output_dir, f"note_{note_id}_basic.json")
                with open(basic_file, 'w', encoding='utf-8') as f:
                    json.dump(processed_note, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… ç¬”è®°åŸºæœ¬ä¿¡æ¯å·²ä¿å­˜: {basic_file}")

            # æ­¥éª¤3: æµå¼è·å–å’Œä¿å­˜è¯„è®º
            if include_comments:
                try:
                    # æå–xsec_token
                    from urllib.parse import urlparse, parse_qs
                    parsed = urlparse(note_url)
                    query_params = parse_qs(parsed.query)
                    xsec_token = query_params.get('xsec_token', [''])[0]

                    if output_dir:
                        # æµå¼ä¿å­˜åˆ°JSONLæ–‡ä»¶
                        comments_file = os.path.join(output_dir, f"note_{note_id}_comments.jsonl")
                        total_comments = self.save_comments_streaming(
                            note_id, xsec_token, comments_file, proxies
                        )
                        processed_note['comment_count'] = total_comments
                        processed_note['comments_file'] = comments_file
                        logger.info(f"âœ… è¯„è®ºæ•°æ®å·²ä¿å­˜: {comments_file} (å…±{total_comments}æ¡)")
                    else:
                        # å…¼å®¹æ—§æ¨¡å¼ï¼šå†…å­˜ä¸­è·å–è¯„è®º
                        logger.info(f'è·å–è¯„è®ºæ•°æ®ï¼ˆæ—§æ¨¡å¼ï¼‰: {note_url}')
                        if self.cookie_pool:
                            success, msg, comments, account = self.get_with_cookie_pool_retry(
                                self.xhs_apis.get_note_all_comment,
                                note_url,
                                proxies=proxies
                            )
                        else:
                            success, msg, comments = self.xhs_apis.get_note_all_comment(
                                note_url, cookies_str, proxies
                            )

                        if success and comments:
                            processed_note['comments'] = comments
                            logger.info(f'è·å–è¯„è®ºæ•°æ®æˆåŠŸï¼Œå…± {len(comments)} æ¡è¯„è®º')
                        else:
                            processed_note['comments'] = []
                            logger.warning(f'è·å–è¯„è®ºæ•°æ®å¤±è´¥: {msg}')

                except Exception as e:
                    processed_note['comments'] = []
                    processed_note['comment_count'] = 0
                    logger.warning(f'è·å–è¯„è®ºæ•°æ®å¼‚å¸¸: {str(e)}')
            else:
                processed_note['comments'] = []
                processed_note['comment_count'] = 0

            # âœ… æ­¥éª¤4: ä¿å­˜æˆ–æ›´æ–°å®Œæ•´ä¿¡æ¯JSONï¼ˆå¦‚æœæŒ‡å®šäº†output_dirï¼‰
            if output_dir:
                full_file = os.path.join(output_dir, f"note_{note_id}_full.json")
                with open(full_file, 'w', encoding='utf-8') as f:
                    json.dump(processed_note, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… ç¬”è®°å®Œæ•´ä¿¡æ¯å·²ä¿å­˜: {full_file}")

            return True, 'è·å–ç¬”è®°å®Œæ•´ä¿¡æ¯æˆåŠŸ', processed_note

        except Exception as e:
            error_msg = f'è·å–ç¬”è®°å®Œæ•´ä¿¡æ¯å¤±è´¥: {str(e)}'
            logger.error(error_msg)
            import traceback
            logger.debug(traceback.format_exc())
            return False, error_msg, None
    
    def process_json_to_full_data(self, json_file_path: str, cookies_str: str, 
                                 output_dir: str = None, include_comments: bool = True, 
                                 download_media: bool = True, save_format: str = 'json',
                                 proxies: dict = None):
        """
        å¤„ç†JSONæ–‡ä»¶ï¼Œè·å–æ‰€æœ‰ç¬”è®°çš„å®Œæ•´ä¿¡æ¯å¹¶ä¿å­˜
        
        :param json_file_path: è¾“å…¥çš„JSONæ–‡ä»¶è·¯å¾„
        :param cookies_str: å°çº¢ä¹¦cookieså­—ç¬¦ä¸²
        :param output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è‡ªåŠ¨ç”Ÿæˆ
        :param include_comments: æ˜¯å¦åŒ…å«è¯„è®ºæ•°æ®
        :param download_media: æ˜¯å¦ä¸‹è½½åª’ä½“æ–‡ä»¶ï¼ˆå›¾ç‰‡ã€è§†é¢‘ï¼‰
        :param save_format: ä¿å­˜æ ¼å¼ 'json', 'excel', 'all'
        :param proxies: ä»£ç†è®¾ç½®
        :return: æˆåŠŸçŠ¶æ€, æ¶ˆæ¯, å¤„ç†ç»“æœç»Ÿè®¡
        """
        try:
            # è§£æJSONæ–‡ä»¶
            parse_success, parse_msg, note_urls = self.parse_json_file(json_file_path)
            if not parse_success:
                return False, parse_msg, {}
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            if output_dir is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                json_filename = os.path.splitext(os.path.basename(json_file_path))[0]
                output_dir = f"full_data_{json_filename}_{timestamp}"
            
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
            
            # åˆ›å»ºåª’ä½“æ–‡ä»¶ç›®å½•
            if download_media:
                media_dir = os.path.join(output_dir, "media_files")
                if not os.path.exists(media_dir):
                    os.makedirs(media_dir)
            
            # å¤„ç†æ¯ä¸ªç¬”è®°
            successful_notes = []
            failed_notes = []
            total_comments_count = 0

            for i, note_url in enumerate(note_urls, 1):
                logger.info(f'æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(note_urls)} ä¸ªç¬”è®°: {note_url}')

                # ä½¿ç”¨æ–°çš„åˆ†æ­¥ä¿å­˜æ–¹æ³•
                success, msg, full_note_info = self.get_note_full_info(
                    note_url,
                    cookies_str=cookies_str,
                    output_dir=output_dir,  # ä¼ é€’output_dirå¯ç”¨åˆ†æ­¥ä¿å­˜
                    proxies=proxies,
                    include_comments=include_comments
                )

                if success and full_note_info:
                    successful_notes.append(full_note_info)

                    # ç»Ÿè®¡è¯„è®ºæ•°
                    comment_count = full_note_info.get('comment_count', 0)
                    total_comments_count += comment_count

                    # ä¸‹è½½åª’ä½“æ–‡ä»¶
                    if download_media:
                        try:
                            download_note(full_note_info, media_dir, 'media')
                            logger.info(f'åª’ä½“æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {full_note_info["title"]}')
                        except Exception as e:
                            logger.warning(f'åª’ä½“æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}')

                    # æ³¨æ„ï¼šå•ä¸ªç¬”è®°çš„JSONæ–‡ä»¶å·²ç»åœ¨get_note_full_infoä¸­ä¿å­˜ï¼Œæ— éœ€é‡å¤ä¿å­˜
                else:
                    failed_notes.append({
                        'url': note_url,
                        'error': msg
                    })
                    logger.error(f'å¤„ç†å¤±è´¥: {msg}')
            
            # ä¿å­˜æ±‡æ€»æ•°æ®
            summary_data = {
                'process_info': {
                    'source_json': json_file_path,
                    'total_notes': len(note_urls),
                    'successful_notes': len(successful_notes),
                    'failed_notes': len(failed_notes),
                    'total_comments': total_comments_count,
                    'include_comments': include_comments,
                    'download_media': download_media,
                    'process_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    'comment_storage': 'JSONL files (*.jsonl)' if include_comments else 'None'
                },
                'successful_notes': successful_notes,
                'failed_notes': failed_notes
            }

            # ä¿å­˜æ±‡æ€»JSONæ–‡ä»¶
            if save_format in ['json', 'all']:
                summary_file = os.path.join(output_dir, "summary_all_notes.json")
                with open(summary_file, 'w', encoding='utf-8') as f:
                    json.dump(summary_data, f, ensure_ascii=False, indent=2)
                logger.success(f'æ±‡æ€»JSONæ–‡ä»¶ä¿å­˜åˆ°: {summary_file}')

            # ä¿å­˜ä¸ºExcelæ ¼å¼
            if save_format in ['excel', 'all']:
                from xhs_utils.data_util import save_to_xlsx

                # ä¿å­˜ç¬”è®°æ•°æ®åˆ°Excel
                if successful_notes:
                    excel_file = os.path.join(output_dir, "notes_data.xlsx")
                    save_to_xlsx(successful_notes, excel_file)
                    logger.success(f'ç¬”è®°Excelæ–‡ä»¶ä¿å­˜åˆ°: {excel_file}')

                # æ³¨æ„ï¼šè¯„è®ºæ•°æ®å·²ä¿å­˜ä¸ºJSONLæ ¼å¼ï¼Œä¸å†è‡ªåŠ¨è½¬æ¢ä¸ºExcel
                # å¦‚éœ€Excelæ ¼å¼ï¼Œå¯æ‰‹åŠ¨è¯»å–JSONLæ–‡ä»¶è½¬æ¢
                if include_comments and total_comments_count > 0:
                    logger.info(f'è¯„è®ºæ•°æ®å·²ä¿å­˜ä¸ºJSONLæ ¼å¼ï¼ˆæ¯ä¸ªç¬”è®°ä¸€ä¸ªæ–‡ä»¶ï¼‰ï¼Œå…± {total_comments_count} æ¡è¯„è®º')
                    logger.info(f'JSONLæ–‡ä»¶ä½ç½®: {output_dir}/note_*_comments.jsonl')
            
            # ä¿å­˜å¤„ç†ç»“æœç»Ÿè®¡
            result_stats = {
                'total_notes': len(note_urls),
                'successful_notes': len(successful_notes),
                'failed_notes': len(failed_notes),
                'success_rate': len(successful_notes) / len(note_urls) * 100 if note_urls else 0,
                'total_comments': total_comments_count,
                'output_directory': output_dir
            }

            logger.success(f'å¤„ç†å®Œæˆï¼æˆåŠŸ: {len(successful_notes)}, å¤±è´¥: {len(failed_notes)}, è¯„è®º: {total_comments_count}æ¡')
            logger.success(f'ç»“æœä¿å­˜åˆ°ç›®å½•: {output_dir}')

            return True, 'å¤„ç†å®Œæˆ', result_stats
            
        except Exception as e:
            error_msg = f'å¤„ç†JSONæ–‡ä»¶å¤±è´¥: {str(e)}'
            logger.error(error_msg)
            return False, error_msg, {}
    
    def batch_process_json_files(self, json_files: list, cookies_str: str, 
                               output_base_dir: str = "batch_full_data", **kwargs):
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªJSONæ–‡ä»¶
        
        :param json_files: JSONæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        :param cookies_str: å°çº¢ä¹¦cookieså­—ç¬¦ä¸²
        :param output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        :param kwargs: å…¶ä»–å¤„ç†å‚æ•°
        :return: æ‰¹é‡å¤„ç†ç»“æœ
        """
        if not os.path.exists(output_base_dir):
            os.makedirs(output_base_dir)
        
        batch_results = []
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for json_file in json_files:
            logger.info(f'å¼€å§‹å¤„ç†JSONæ–‡ä»¶: {json_file}')
            
            # ä¸ºæ¯ä¸ªJSONæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
            json_name = os.path.splitext(os.path.basename(json_file))[0]
            output_dir = os.path.join(output_base_dir, f"{json_name}_{timestamp}")
            
            success, msg, stats = self.process_json_to_full_data(
                json_file, cookies_str, output_dir, **kwargs
            )
            
            batch_results.append({
                'json_file': json_file,
                'success': success,
                'message': msg,
                'stats': stats,
                'output_dir': output_dir if success else None
            })
        
        # ä¿å­˜æ‰¹é‡å¤„ç†æ±‡æ€»ç»“æœ
        batch_summary = {
            'batch_info': {
                'total_files': len(json_files),
                'successful_files': len([r for r in batch_results if r['success']]),
                'total_notes_processed': sum([r['stats'].get('total_notes', 0) for r in batch_results]),
                'total_successful_notes': sum([r['stats'].get('successful_notes', 0) for r in batch_results]),
                'process_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            },
            'results': batch_results
        }
        
        batch_summary_file = os.path.join(output_base_dir, f"batch_summary_{timestamp}.json")
        with open(batch_summary_file, 'w', encoding='utf-8') as f:
            json.dump(batch_summary, f, ensure_ascii=False, indent=2)
        
        logger.success(f'æ‰¹é‡å¤„ç†å®Œæˆï¼Œæ±‡æ€»ç»“æœä¿å­˜åˆ°: {batch_summary_file}')
        return batch_results


if __name__ == '__main__':
    """
    è§£æJSONæ–‡ä»¶å¹¶è·å–å®Œæ•´ç¬”è®°ä¿¡æ¯çš„ç¤ºä¾‹ä½¿ç”¨
    """
    # åˆå§‹åŒ–é…ç½®
    cookies_str, base_path = init()
    json_processor = JsonToFullData()
    
    # ç¤ºä¾‹ï¼šå¤„ç†å•ä¸ªJSONæ–‡ä»¶
    json_file_path = "search_results/search_æ—¥æœ¬æ–™ç†_20250905_183800.json"  # è¯·ä¿®æ”¹ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if os.path.exists(json_file_path):
        success, msg, stats = json_processor.process_json_to_full_data(
            json_file_path=json_file_path,
            cookies_str=cookies_str,
            include_comments=True,  # åŒ…å«è¯„è®ºæ•°æ®
            download_media=True,    # ä¸‹è½½åª’ä½“æ–‡ä»¶
            save_format='all'       # ä¿å­˜ä¸ºJSONå’ŒExcelæ ¼å¼
        )
        
        if success:
            print(f"âœ… å¤„ç†æˆåŠŸ: {msg}")
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}")
        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {msg}")
    else:
        print(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_file_path}")
        print("è¯·å…ˆè¿è¡Œ search_to_json.py ç”ŸæˆJSONæ–‡ä»¶")
    
    # æ‰¹é‡å¤„ç†ç¤ºä¾‹ï¼ˆæ³¨é‡Šæ‰ï¼Œéœ€è¦æ—¶å¯ç”¨ï¼‰
    """
    json_files = [
        "search_results/search_æ—¥æœ¬æ–™ç†_20240101_120000.json",
        "search_results/search_æ„å¤§åˆ©é¢_20240101_120000.json"
    ]
    
    existing_files = [f for f in json_files if os.path.exists(f)]
    if existing_files:
        results = json_processor.batch_process_json_files(
            json_files=existing_files,
            cookies_str=cookies_str,
            include_comments=True,
            download_media=True,
            save_format='all'
        )
        
        for result in results:
            status = "âœ…" if result['success'] else "âŒ"
            file_name = os.path.basename(result['json_file'])
            print(f"{status} {file_name}: {result['message']}")
    """